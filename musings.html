<!DOCTYPE html>

<html>
    <head>
        <title>Himansu Didwania</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"> 
        <link href="css/page.css" rel="stylesheet">
        <link href="css/topbar.css" rel="stylesheet">
        <link href="css/musings.css" rel="stylesheet">
    </head>

    <body>
        <div id='topbar'>
            <div id='heading'>
                Himansu Didwania
            </div>
            <div id='options'>
                <div id='button' onclick="location.href = 'index.html';">about</div>
                <div id='button' class="selected" onclick="location.href = 'musings.html';">musings</div>
            </div>
        </div>

        <div id='musings'>
            <h1>Thoughts, Reflections and Realisations</h1>
            <hr>
            <p>
                <b>May 05, 2022</b> :
                <br>
                I am trying to immplement a neural network from <a href="https://github.com/hdidwania/ml-from-scratch">scratch</a>. Forward pass, backwardprop, all
                just using NumPy. In my implementation, I was trying to use batch size as the first dimension of my vectors, e.g. an input vector in form of <code>(b, 128)</code>.
                But something seemed wrong with this. After some thought, a realisation hit. Using this format breaks down the general intuition of Linear Algebra in some sense.
                This notation treats every sample as a row matrix, rather than a vertical vector. And from here, everything starts breaking down. Now the weight matrix should be on
                the right side; <code>y = np.dot(x, W)</code>, which doesn't translate to the view that <code>W</code> is <i>acting upon</i> <code>x</code>.
                Although some implementation trick can work with this, but I really liked this realisation, and understood how the index representing batch can make
                such a difference.
            </p>
            <hr>
        </div>
    </body>
</html>